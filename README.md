# Data Warehousing with AWS

## Summary

Sparkify, a music streaming company, would like to understand what music their users are listening to, in order to plan the direction of their growing business.  To help answer this, and any future analytical questions, they decided to build a data warehouse using song and event data files.  Sparkify uses AWS as their cloud provider, storing data in S3 buckets.  They would like to use a Redshift cluster as their Data Warehouse.  This project contains data, implementation code, as well as documentation.

## Data
* **Song Dataset**
This dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song.  The data resides in S3 bucket here: s3://udacity-dend/log_data.

* **Log Dataset**
This dataset consists of log files in JSON format generated by an event simulator based on the songs in the Song dataset.  Data resides in S3 bucket here: s3://udacity-dend/song_data.

## Data Flow
![alt text](https://github.com/negmatm/udacity-data-engineering-project3/blob/main/images/Data%20Flow.png?raw=true)

## Data Model
### Staging
![alt text](https://github.com/negmatm/udacity-data-engineering-project3/blob/main/images/Staging%20ERD.png?raw=true)

### Data Warehouse
![alt text](https://github.com/negmatm/udacity-data-engineering-project3/blob/main/images/DW%20ERD.png?raw=true)

## Configurations
* **dhw.cfg:** Configuration file, which contains information about the cluster (host/dbname/user/password/port), iam role (arn), and s3 (log data/log json path/song data) 

## Scripts
* **sql_queries.py:** Defines all the needed SQL queries
* **create_tables.py:** Contains the code to run queries within sql_queries.py to create needed staging/fact/dimension tables
* **etl.py:** Main ETL program, which processes log files, and loads the data into staging and then fact/dimension tables

## Data Processing
#### Assumptions
1. IAM user has been created in AWS
2. IAM role has been created, which will allows redshift to access s3 (read only)
3. Redshift cluster has been created
4. Incoming TCP port to access cluster endpoint has been opened
5. dwh.cfg file has been properly populated

#### Steps
1. Run **create_tables.py** to create staging/dimension/fact tables
2. Run **etl.py** to read the files from s3 bucket, load them into staging tables, and then process them into fact/dimension tables on the Redshift cluster
